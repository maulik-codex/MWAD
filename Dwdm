1.Create a Star Schema
Design and implement a star schema for a retail business scenario including fact and dimension tables. Populate it with sample data.(12/07/2025)
CREATE TABLE Product_Dim (
    product_id INT PRIMARY KEY,
    product_name VARCHAR(50),
    category VARCHAR(30),
    brand VARCHAR(30),
    price DECIMAL(10.2)
);
Customer Dimension
CREATE TABLE Customer_Dim (
    customer_id INT PRIMARY KEY,
    customer_name VARCHAR(50),
    gender CHAR(1),
    age INT,
    city VARCHAR(50)
);

CREATE TABLE Store_Dim (
    store_id INT PRIMARY KEY,
    store_name VARCHAR(50),
    location VARCHAR(50),
    manager_name VARCHAR(50)
);

CREATE TABLE Time_Dim (
    time_id INT PRIMARY KEY,
    date DATE,
    month VARCHAR(15),
    quarter VARCHAR(10),
    year INT
);

CREATE TABLE Sales_Fact (
    sale_id INT PRIMARY KEY,
    product_id INT,
    customer_id INT,
    store_id INT,
    time_id INT,
    quantity_sold INT,
    total_amount DECIMAL(10,2),
    FOREIGN KEY (product_id) REFERENCES Product_Dim(product_id),
    FOREIGN KEY (customer_id) REFERENCES Customer_Dim(customer_id),
    FOREIGN KEY (store_id) REFERENCES Store_Dim(store_id),
    FOREIGN KEY (time_id) REFERENCES Time_Dim(time_id)
);
INSERT INTO Product_Dim VALUES
(1, 'Laptop', 'Electronics', 'HP', 50000),
(2, 'Smartphone', 'Electronics', 'Samsung', 25000),
(3, 'Shampoo', 'Personal Care', 'Dove', 250);
INSERT INTO Customer_Dim VALUES
(101, 'John Doe', 'M', 32, 'Chennai'),
(102, 'Jane Smith', 'F', 27, 'Bangalore'),
(103, 'Ali Khan', 'M', 45, 'Mumbai');
INSERT INTO Store_Dim VALUES
(1, 'BigMart Chennai', 'Chennai', 'Mr. Raj'),
(2, 'BigMart Mumbai', 'Mumbai', 'Mrs. Meera');
INSERT INTO Time_Dim VALUES
(1001, '2024-12-01', 'December', 'Q4', 2024),
(1002, '2025-01-15', 'January', 'Q1', 2025);
INSERT INTO Sales_Fact VALUES
(1, 1, 101, 1, 1001, 1, 50000),
(2, 2, 102, 2, 1002, 2, 50000),
(3, 3, 103, 1, 1002, 4, 1000);
SELECT p.product_name, SUM(s.total_amount) AS revenue
FROM Sales_Fact s
JOIN Product_Dim p ON s.product_id = p.product_id
GROUP BY p.product_name;
SELECT t.month, SUM(s.total_amount) AS monthly_revenue
FROM Sales_Fact s
JOIN Time_Dim t ON s.time_id = t.time_id
GROUP BY t.month;


2.ETL Process Using SQL 
Perform an ETL (Extract, Transform, Load) process using sample sales data. Clean and transform the data before loading it into a warehouse.
1.Extract ‚Äì Get Data from Source
Let‚Äôs assume we have 3 raw CSV files:
raw_sales.csv
raw_customers.csv
raw_products.csv
Sample: raw_sales.csv
sale_id
customer_name
product_name
quantity
price
date
store
1
John Doe
Laptop
1
50000
2024-12-01
Chennai
2
Jane Smith
Shampoo
4
250
2025-01-15
Mumbai
3
NULL
Laptop
1
50000
NULL
Chennai

2. Transform ‚Äì Clean and Prepare Data
This step includes:
Removing rows with NULLs in essential fields
Converting date formats
Removing duplicates
Splitting data into dimensions
Creating surrogate keys
Example Transformation in SQL (or Python/Pandas/ETL tool)
SQL-style transformation for sales data (if loaded into staging table):
Remove rows with nullsCREATE TABLE Cleaned_Sales ASSELECT * FROM Staging_SalesWHERE customer_name IS NOT NULL
  AND product_name IS NOT NULL
  AND date IS NOT NULL;
Remove duplicatesDELETE FROM Cleaned_SalesWHERE sale_id NOT IN (
  SELECT MIN(sale_id)
  FROM Cleaned_Sales
  GROUP BY customer_name, product_name, date
);
-- Format dateUPDATE Cleaned_SalesSET date = STR_TO_DATE(date, '%Y-%m-%d');

3.Load ‚Äì Insert into Data Warehouse Tables
Assume the star schema has already been created with these tables:
Product_Dim
Customer_Dim
Store_Dim
Time_Dim
Sales_Fact
3.1 Load Dimensions
-- Customer DimensionINSERT INTO Customer_Dim (customer_id, customer_name, gender, age, city)SELECT DISTINCT ROW_NUMBER() OVER () AS customer_id,
       customer_name, 'F', 30, 'Mumbai'FROM Cleaned_Sales;
-- Product DimensionINSERT INTO Product_Dim (product_id, product_name, category, brand, price)SELECT DISTINCT ROW_NUMBER() OVER () AS product_id,
       product_name, 'Electronics', 'DefaultBrand', priceFROM Cleaned_Sales;
-- Store DimensionINSERT INTO Store_Dim (store_id, store_name, location, manager_name)SELECT DISTINCT ROW_NUMBER() OVER () AS store_id,
       CONCAT('Store_', store), store, 'Default Manager'FROM Cleaned_Sales;
-- Time DimensionINSERT INTO Time_Dim (time_id, date, month, quarter, year)SELECT DISTINCT
       ROW_NUMBER() OVER () AS time_id,
       date,
       MONTHNAME(date),
       CONCAT('Q', QUARTER(date)),
       YEAR(date)FROM Cleaned_Sales;

3.2 Load Fact Table
Join the dimensions using lookups and populate the fact table:
INSERT INTO Sales_Fact (sale_id, product_id, customer_id, store_id, time_id, quantity_sold, total_amount)SELECT 
    s.sale_id,
    p.product_id,
    c.customer_id,
    st.store_id,
    t.time_id,
    s.quantity,
    s.quantity * s.priceFROM Cleaned_Sales sJOIN Product_Dim p ON s.product_name = p.product_nameJOIN Customer_Dim c ON s.customer_name = c.customer_nameJOIN Store_Dim st ON s.store = st.locationJOIN Time_Dim t ON s.date = t.date;

3.Build a Data Cube
Use SQL Server Analysis Services (SSAS) or Oracle to create a data cube for product sales and perform slicing and dicing operations.
Step 1: Prepare the Data in SQL Server
Create the tables: DimProduct, DimDate, and FactSales.
-- Dimension Table: ProductCREATE TABLE DimProduct (
    ProductID INT PRIMARY KEY,
    ProductName VARCHAR(100),
    Category VARCHAR(50)
);
-- Dimension Table: DateCREATE TABLE DimDate (
    DateID INT PRIMARY KEY,
    FullDate DATE,
    MonthName VARCHAR(20),
    Quarter VARCHAR(10),
    Year INT
);
-- Fact Table: SalesCREATE TABLE FactSales (
    SalesID INT PRIMARY KEY,
    ProductID INT FOREIGN KEY REFERENCES DimProduct(ProductID),
    DateID INT FOREIGN KEY REFERENCES DimDate(DateID),
    SalesAmount DECIMAL(10, 2),
    QuantitySold INT
);
Insert some sample data 
INSERT INTO DimProduct VALUES
(1, 'Laptop', 'Electronics'),
(2, 'Headphones', 'Accessories'),
(3, 'Desk Chair', 'Furniture');
INSERT INTO DimDate VALUES
(101, '2025-01-15', 'January', 'Q1', 2025),
(102, '2025-02-20', 'February', 'Q1', 2025),
(103, '2025-04-10', 'April', 'Q2', 2025);
INSERT INTO FactSales VALUES
(1001, 1, 101, 1000.00, 2),
(1002, 2, 102, 150.00, 5),
(1003, 3, 103, 250.00, 1);

‚öôÔ∏è Step 2: Create SSAS Multidimensional Project in SSDT (Visual Studio)
Open SQL Server Data Tools (SSDT).
Create a new project:
Type: Analysis Services Multidimensional and Data Mining Project.
üîß Define Data Source
Connect to your SQL Server database where tables are stored.
Create Data Source View (DSV)
Add DimProduct, DimDate, FactSales to DSV.
Ensure relationships are correctly set:
FactSales.ProductID ‚Üí DimProduct.ProductID
FactSales.DateID ‚Üí DimDate.DateID

Step 3: Define Dimensions
Create two dimensions:
Product Dimension
Attributes: ProductName, Category
Date Dimension
Attributes: MonthName, Quarter, Year

Step 4: Create the Cube
Create new cube.
Select FactSales as Measure Group.
Measures: SalesAmount, QuantitySold
Add Dimensions: Product, Date

Step 5: Deploy and Process the Cube
Right-click the project ‚Üí Deploy.
Right-click ‚Üí Process the cube (load data into the cube).
Then use SQL Server Management Studio (SSMS) to browse the cube.

Step 6: Perform Slicing and Dicing (Using MDX)
Slice: Get total sales for ‚ÄúLaptop‚Äù only
SELECT 
  {[Measures].[SalesAmount]} ON COLUMNS,
  {[DimProduct].[ProductName].&[Laptop]} ON ROWS
FROM [SalesCube]
Dice: Get sales by product and year
SELECT 
  {[Measures].[SalesAmount]} ON COLUMNS,
  NON EMPTY 
  CrossJoin(
    [DimProduct].[ProductName].[ProductName].Members, 
    [DimDate].[Year].[Year].Members
  ) ON ROWS
FROM [SalesCube]

Write OLAP Queries
Use SQL to perform OLAP operations (Roll-up, Drill-down, Slice, Dice, Pivot) on a data warehouse built from a movie rental dataset.
Dimension Tables
-- Movie DimensionCREATE TABLE DimMovie (
    MovieID INT PRIMARY KEY,
    Title VARCHAR(100),
    Genre VARCHAR(50),
    ReleaseYear INT
);
-- Customer DimensionCREATE TABLE DimCustomer (
    CustomerID INT PRIMARY KEY,
    Name VARCHAR(100),
    City VARCHAR(50),
    AgeGroup VARCHAR(20)
);
-- Date DimensionCREATE TABLE DimDate (
    DateID INT PRIMARY KEY,
    FullDate DATE,
    MonthName VARCHAR(20),
    Year INT,
    Quarter VARCHAR(10)
);
Fact Table
-- Rental Fact TableCREATE TABLE FactRental (
    RentalID INT PRIMARY KEY,
    MovieID INT,
    CustomerID INT,
    DateID INT,
    RentalAmount DECIMAL(8,2),
    FOREIGN KEY (MovieID) REFERENCES DimMovie(MovieID),
    FOREIGN KEY (CustomerID) REFERENCES DimCustomer(CustomerID),
    FOREIGN KEY (DateID) REFERENCES DimDate(DateID)
);

OLAP Operations with SQL
1.ROLL-UP
Total rental amount rolled up by Genre ‚Üí Year
SELECT 
    Genre, 
    Year, 
    SUM(RentalAmount) AS TotalRentalFROM FactRental FJOIN DimMovie M ON F.MovieID = M.MovieIDJOIN DimDate D ON F.DateID = D.DateIDGROUP BY ROLLUP (Genre, Year);

2.DRILL-DOWN
Drill down from Genre to specific Movies in 2025
SELECT 
    Genre, 
    Title, 
    SUM(RentalAmount) AS TotalRentalFROM FactRental FJOIN DimMovie M ON F.MovieID = M.MovieIDJOIN DimDate D ON F.DateID = D.DateIDWHERE D.Year = 2025GROUP BY Genre, Title;

3.SLICE
Slice the data for rentals made in 'Action' genre only
SELECT 
    Title, 
    SUM(RentalAmount) AS TotalRentalFROM FactRental FJOIN DimMovie M ON F.MovieID = M.MovieIDWHERE Genre = 'Action'GROUP BY Title;

4.DICE
Get rental amounts for ‚ÄòComedy‚Äô or ‚ÄòAction‚Äô genres AND customers in the age group '18-25' during the year 2025
SELECT 
    Genre, 
    AgeGroup, 
    Year,
    SUM(RentalAmount) AS TotalRentalFROM FactRental FJOIN DimMovie M ON F.MovieID = M.MovieIDJOIN DimCustomer C ON F.CustomerID = C.CustomerIDJOIN DimDate D ON F.DateID = D.DateIDWHERE Genre IN ('Comedy', 'Action')
  AND AgeGroup = '18-25'
  AND Year = 2025GROUP BY Genre, AgeGroup, Year;

5.PIVOT
Pivot table: Total rental amount per Genre per Year
SQL Server example using PIVOT operator
SELECT *FROM (
    SELECT 
        Genre, 
        Year, 
        RentalAmount
    FROM FactRental F
    JOIN DimMovie M ON F.MovieID = M.MovieID
    JOIN DimDate D ON F.DateID = D.DateID
) AS SourceTable
PIVOT (
    SUM(RentalAmount)
    FOR Year IN ([2023], [2024], [2025])
) AS PivotTable;
For PostgreSQL or Oracle, use CASE WHEN style pivot:
SELECT 
    Genre,
    SUM(CASE WHEN Year = 2023 THEN RentalAmount ELSE 0 END) AS "2023",
    SUM(CASE WHEN Year = 2024 THEN RentalAmount ELSE 0 END) AS "2024",
    SUM(CASE WHEN Year = 2025 THEN RentalAmount ELSE 0 END) AS "2025"FROM FactRental FJOIN DimMovie M ON F.MovieID = M.MovieIDJOIN DimDate D ON F.DateID = D.DateIDGROUP BY Genre;





5.Design and Implement a Mini Data Warehouse
Choose a domain (e.g., hospital, university, airline) and build a schema including fact and dimension tables.
Create Dimension Tables
-- Patient DimensionCREATE TABLE DimPatient (
    PatientID INT PRIMARY KEY,
    Name VARCHAR(100),
    Gender CHAR(1),
    Age INT,
    City VARCHAR(50)
);
-- Doctor DimensionCREATE TABLE DimDoctor (
    DoctorID INT PRIMARY KEY,
    DoctorName VARCHAR(100),
    Specialization VARCHAR(50)
);
-- Department DimensionCREATE TABLE DimDepartment (
    DepartmentID INT PRIMARY KEY,
    DepartmentName VARCHAR(50)
);
-- Date DimensionCREATE TABLE DimDate (
    DateID INT PRIMARY KEY,
    FullDate DATE,
    Day INT,
    Month INT,
    Quarter VARCHAR(10),
    Year INT
);
-- Diagnosis DimensionCREATE TABLE DimDiagnosis (
    DiagnosisID INT PRIMARY KEY,
    DiagnosisName VARCHAR(100),
    Treatment VARCHAR(100)
);

2.Create Fact Table
CREATE TABLE FactPatientVisits (
    VisitID INT PRIMARY KEY,
    PatientID INT,
    DoctorID INT,
    DepartmentID INT,
    DateID INT,
    DiagnosisID INT,
    BillAmount DECIMAL(10, 2),

    FOREIGN KEY (PatientID) REFERENCES DimPatient(PatientID),
    FOREIGN KEY (DoctorID) REFERENCES DimDoctor(DoctorID),
    FOREIGN KEY (DepartmentID) REFERENCES DimDepartment(DepartmentID),
    FOREIGN KEY (DateID) REFERENCES DimDate(DateID),
    FOREIGN KEY (DiagnosisID) REFERENCES DimDiagnosis(DiagnosisID)
);

3.Sample Data Insertion
-- PatientsINSERT INTO DimPatient VALUES
(1, 'John Doe', 'M', 35, 'Chennai'),
(2, 'Jane Smith', 'F', 29, 'Delhi');
-- DoctorsINSERT INTO DimDoctor VALUES
(101, 'Dr. Kumar', 'Cardiology'),
(102, 'Dr. Meena', 'Neurology');
-- DepartmentsINSERT INTO DimDepartment VALUES
(201, 'Cardiology'),
(202, 'Neurology');
-- DatesINSERT INTO DimDate VALUES
(301, '2025-07-01', 1, 7, 'Q3', 2025),
(302, '2025-07-15', 15, 7, 'Q3', 2025);
-- DiagnosesINSERT INTO DimDiagnosis VALUES
(401, 'Hypertension', 'Medication'),
(402, 'Migraine', 'Therapy');
-- Fact TableINSERT INTO FactPatientVisits VALUES
(501, 1, 101, 201, 301, 401, 1500.00),
(502, 2, 102, 202, 302, 402, 2000.00);

4.Example Queries (OLAP-style)
üîπ Total billing by department:
SELECT D.DepartmentName, SUM(F.BillAmount) AS TotalRevenueFROM FactPatientVisits FJOIN DimDepartment D ON F.DepartmentID = D.DepartmentIDGROUP BY D.DepartmentName;
Number of patients per diagnosis:
SELECT Di.DiagnosisName, COUNT(DISTINCT F.PatientID) AS PatientCountFROM FactPatientVisits FJOIN DimDiagnosis Di ON F.DiagnosisID = Di.DiagnosisIDGROUP BY Di.DiagnosisName;
Revenue by doctor per quarter:
SELECT Doc.DoctorName, Da.Quarter, SUM(F.BillAmount) AS RevenueFROM FactPatientVisits FJOIN DimDoctor Doc ON F.DoctorID = Doc.DoctorIDJOIN DimDate Da ON F.DateID = Da.DateIDGROUP BY Doc.DoctorName, Da.Quarter;
6.Create a Snowflake Schema
Design and implement a snowflake schema for a university admission system, and compare it with a star schema.
Domain: University Admission System
Goal:
Track and analyze student admissions, departments, programs, locations, and time dimensions.

1.Snowflake Schema Design
Fact Table:
FactAdmissions: Stores admission-related facts like number of students, admission score, etc.
Dimension Tables (Snowflaked):
DimStudent (normalized with DimLocation)
DimProgram (normalized with DimDepartment, DimFaculty)
DimDate

‚ùÑÔ∏è Snowflake Schema Diagram (Text Representation)
               +----------------+
               |  DimLocation   |
               +----------------+
                        ‚ñ≤
                        |
              +----------------+
              |  DimStudent    |
              +----------------+
                        ‚ñ≤
                        |
+----------------+     |     +------------------+
| DimDepartment  |<----+---->|   DimProgram     |
+----------------+           +------------------+
         ‚ñ≤                            ‚ñ≤
         |                            |
  +---------------+           +------------------+
  |  DimFaculty   |           |    DimDate       |
  +---------------+           +------------------+

               --------------------------
               |     FactAdmissions     |
               --------------------------

2. SQL Code for Snowflake Schema
2.1 Dimension Tables
üåç DimLocation
CREATE TABLE DimLocation (
    LocationID INT PRIMARY KEY,
    City VARCHAR(50),
    State VARCHAR(50),
    Country VARCHAR(50)
);
DimStudent
CREATE TABLE DimStudent (
    StudentID INT PRIMARY KEY,
    StudentName VARCHAR(100),
    Gender CHAR(1),
    Age INT,
    LocationID INT,
    FOREIGN KEY (LocationID) REFERENCES DimLocation(LocationID)
);
DimFaculty
CREATE TABLE DimFaculty (
    FacultyID INT PRIMARY KEY,
    FacultyName VARCHAR(100)
);
DimDepartment
CREATE TABLE DimDepartment (
    DepartmentID INT PRIMARY KEY,
    DepartmentName VARCHAR(100),
    FacultyID INT,
    FOREIGN KEY (FacultyID) REFERENCES DimFaculty(FacultyID)
);
DimProgram
CREATE TABLE DimProgram (
    ProgramID INT PRIMARY KEY,
    ProgramName VARCHAR(100),
    Level VARCHAR(50),  -- UG/PG
    DepartmentID INT,
    FOREIGN KEY (DepartmentID) REFERENCES DimDepartment(DepartmentID)
);
DimDate
CREATE TABLE DimDate (
    DateID INT PRIMARY KEY,
    FullDate DATE,
    Month INT,
    Quarter VARCHAR(10),
    Year INT
);

2.2 Fact Table
CREATE TABLE FactAdmissions (
    AdmissionID INT PRIMARY KEY,
    StudentID INT,
    ProgramID INT,
    DateID INT,
    AdmissionScore DECIMAL(5,2),
    ScholarshipAmount DECIMAL(10,2),

    FOREIGN KEY (StudentID) REFERENCES DimStudent(StudentID),
    FOREIGN KEY (ProgramID) REFERENCES DimProgram(ProgramID),
    FOREIGN KEY (DateID) REFERENCES DimDate(DateID)
);

3. Sample Inserts
-- LocationINSERT INTO DimLocation VALUES (1, 'Chennai', 'Tamil Nadu', 'India');
-- StudentINSERT INTO DimStudent VALUES (1001, 'Arjun R', 'M', 18, 1);
-- FacultyINSERT INTO DimFaculty VALUES (10, 'Engineering');
-- DepartmentINSERT INTO DimDepartment VALUES (20, 'Computer Science', 10);
-- ProgramINSERT INTO DimProgram VALUES (30, 'B.Tech CS', 'UG', 20);
-- DateINSERT INTO DimDate VALUES (101, '2025-06-20', 6, 'Q2', 2025);
-- AdmissionINSERT INTO FactAdmissions VALUES (5001, 1001, 30, 101, 87.5, 20000);

4. Example OLAP Queries
Total admissions by department:
SELECT D.DepartmentName, COUNT(*) AS TotalAdmissionsFROM FactAdmissions FJOIN DimProgram P ON F.ProgramID = P.ProgramIDJOIN DimDepartment D ON P.DepartmentID = D.DepartmentIDGROUP BY D.DepartmentName;
Average score by city:
SELECT L.City, AVG(F.AdmissionScore) AS AvgScoreFROM FactAdmissions FJOIN DimStudent S ON F.StudentID = S.StudentIDJOIN DimLocation L ON S.LocationID = L.LocationIDGROUP BY L.City;

7.Performance Tuning in Data Warehouse Queries
Write SQL queries for large datasets and optimize them using indexing, partitioning, or materialized views.
Example: Large Fact Table ‚Äì FactSales
-- Time dimensionCREATE TABLE DimDate (
    DateID INT PRIMARY KEY,
    FullDate DATE,
    Month INT,
    Quarter VARCHAR(10),
    Year INT
);
-- Product dimensionCREATE TABLE DimProduct (
    ProductID INT PRIMARY KEY,
    ProductName VARCHAR(100),
    Category VARCHAR(50)
);
-- Customer dimensionCREATE TABLE DimCustomer (
    CustomerID INT PRIMARY KEY,
    Name VARCHAR(100),
    Region VARCHAR(50)
);
-- Large Fact TableCREATE TABLE FactSales (
    SaleID BIGINT PRIMARY KEY,
    ProductID INT,
    CustomerID INT,
    DateID INT,
    QuantitySold INT,
    SaleAmount DECIMAL(12,2),
    
    FOREIGN KEY (ProductID) REFERENCES DimProduct(ProductID),
    FOREIGN KEY (CustomerID) REFERENCES DimCustomer(CustomerID),
    FOREIGN KEY (DateID) REFERENCES DimDate(DateID)
);

Step 1: Write Expensive SQL Query
This query calculates total sales per product category per year:
SELECT 
    P.Category,
    D.Year,
    SUM(F.SaleAmount) AS TotalRevenueFROM FactSales FJOIN DimProduct P ON F.ProductID = P.ProductIDJOIN DimDate D ON F.DateID = D.DateIDGROUP BY P.Category, D.Year;

Step 2: Performance Tuning
‚úÖ 1. Indexing
Add indexes on commonly filtered or joined columns:
-- Index for joinsCREATE INDEX idx_factsales_productid ON FactSales(ProductID);CREATE INDEX idx_factsales_dateid ON FactSales(DateID);
-- Optional index on amount for aggregationsCREATE INDEX idx_factsales_saleamount ON FactSales(SaleAmount);
-- On dimension keysCREATE INDEX idx_dimproduct_category ON DimProduct(Category);CREATE INDEX idx_dimdate_year ON DimDate(Year);
üîç Use EXPLAIN PLAN (Oracle/PostgreSQL) or SET STATISTICS IO ON (SQL Server) to confirm improvement.

‚úÖ 2. Partitioning
Partition the fact table by year or date range.
SQL Server: Partition by DateID or Year
-- Create partition function (by year)CREATE PARTITION FUNCTION pf_YearRange (INT)AS RANGE LEFT FOR VALUES (2022, 2023, 2024, 2025);
-- Create partition schemeCREATE PARTITION SCHEME ps_YearRangeAS PARTITION pf_YearRange ALL TO ([PRIMARY]);
-- Create partitioned table (re-create FactSales)CREATE TABLE FactSalesPartitioned (
    SaleID BIGINT PRIMARY KEY,
    ProductID INT,
    CustomerID INT,
    DateID INT,
    QuantitySold INT,
    SaleAmount DECIMAL(12,2)
) ON ps_YearRange(DateID);
üöÄ Result: Queries that filter by DateID or Year will only scan the relevant partition.

‚úÖ 3. Materialized View (MV)
Precompute and store the result of expensive aggregations.
Oracle or PostgreSQL:
CREATE MATERIALIZED VIEW mv_sales_summary
BUILD IMMEDIATE
REFRESH ON DEMANDASSELECT 
    P.Category,
    D.Year,
    SUM(F.SaleAmount) AS TotalRevenueFROM FactSales FJOIN DimProduct P ON F.ProductID = P.ProductIDJOIN DimDate D ON F.DateID = D.DateIDGROUP BY P.Category, D.Year;
Now query this instead of the raw tables:
SELECT * FROM mv_sales_summary WHERE Year = 2025;

8.Data Preprocessing in WEKA or Python (Pandas)
Load a dataset (e.g., Titanic or Iris), handle missing values, normalize data, and discretize continuous attributes.

Data Preprocessing in WEKA
WEKA has a GUI called Explorer, which makes preprocessing simple.
Steps:
Load Dataset
Open WEKA ‚Üí Explorer ‚Üí "Preprocess" tab.
Load a dataset (e.g., iris.arff or titanic.arff).
Handle Missing Values
Go to Filter ‚Üí Unsupervised ‚Üí Attribute ‚Üí ReplaceMissingValues.
Apply ‚Üí missing values are replaced with mean (numeric) or mode (categorical).
Normalize Data
Filter ‚Üí Unsupervised ‚Üí Attribute ‚Üí Normalize.
This scales values between 0 and 1.
Discretize Continuous Attributes
Filter ‚Üí Unsupervised ‚Üí Attribute ‚Üí Discretize.
Choose number of bins (e.g., 5 bins).
This converts continuous attributes into categorical intervals.
‚úÖ After preprocessing, you can save the processed dataset in .arff or .csv.
9.Association Rule Mining
Use the Apriori algorithm in WEKA or Python (mlxtend) to find interesting rules in a supermarket transaction dataset.
1. Using Apriori in WEKA
WEKA has Apriori built-in under Associate tab.
Steps:
Load Dataset
Open WEKA ‚Üí Explorer ‚Üí Preprocess tab.
Load a dataset in ARFF or CSV format (e.g., supermarket.arff).
Transactions should be represented in a basket format (e.g., each row = one customer‚Äôs purchases).
Example dataset snippet (ARFF):
@relation supermarket
@attribute milk {t,f}
@attribute bread {t,f}
@attribute butter {t,f}
@attribute cheese {t,f}
@data
t,t,f,f
t,t,t,f
f,t,t,t
t,f,t,f
Run Apriori
Go to Associate tab.
Choose Apriori as the algorithm.
Set parameters:
minMetric (confidence threshold, e.g., 0.8)
lowerBoundMinSupport (support threshold, e.g., 0.1).
View Rules
WEKA outputs association rules like:
butter=t  ==> bread=t  conf:(0.9)milk=t    ==> bread=t  conf:(0.85)
10.Decision Tree Classification
Build a decision tree using ID3 or J48 algorithm on a sample dataset (e.g., weather or bank loan data). Visualize the tree and analyze results.
Decision Tree in WEKA (J48)
Example Dataset: Weather.arff
@relation weather
@attribute outlook {sunny, overcast, rainy}
@attribute temperature {hot, mild, cool}
@attribute humidity {high, normal}
@attribute windy {TRUE, FALSE}
@attribute play {yes, no}

@data
sunny,hot,high,FALSE,no
sunny,hot,high,TRUE,no
overcast,hot,high,FALSE,yes
rainy,mild,high,FALSE,yes
rainy,cool,normal,FALSE,yes
rainy,cool,normal,TRUE,no
overcast,cool,normal,TRUE,yes
sunny,mild,high,FALSE,no
sunny,cool,normal,FALSE,yes
rainy,mild,normal,FALSE,yes
sunny,mild,normal,TRUE,yes
overcast,mild,high,TRUE,yes
overcast,hot,normal,FALSE,yes
rainy,mild,high,TRUE,no
Steps in WEKA:
Open Explorer ‚Üí Preprocess tab. Load weather.arff.
Go to Classify tab ‚Üí Choose J48 (weka.classifiers.trees.J48).
Set parameters: -C 0.25 (confidence factor), -M 2 (min num instances per leaf).


11.Clustering with K-Means
Apply the K-Means algorithm to cluster a dataset (e.g., customer segmentation) and visualize the clusters
1. K-Means in WEKA
WEKA provides SimpleKMeans under the Cluster tab.
Steps:
Open WEKA ‚Üí Explorer ‚Üí Preprocess.
Load a dataset (e.g., iris.arff or customer purchase dataset).
Go to the Cluster tab.
Choose SimpleKMeans (weka.clusterers.SimpleKMeans).
Set number of clusters (e.g., k = 3).
You can choose distance function (default: Euclidean).
Run clustering ‚Üí WEKA will output cluster centroids and assign each instance to a cluster.
Click Visualize ‚Üí Scatter plot of clusters.
‚úÖ Example (Iris dataset):
KMeans finds 3 clusters corresponding roughly to the 3 species of iris flowers.
12.Naive Bayes Classification
Use WEKA or Python to apply Naive Bayes on a spam/ham email dataset. Evaluate the model using confusion matrix.
Na√Øve Bayes in WEKA
Steps:
Load Dataset
Open WEKA ‚Üí Explorer ‚Üí Preprocess tab.
Load a spam dataset (e.g., spam.arff from UCI SpamBase or email word frequency dataset).
Example attributes: word frequencies, capital letters count, etc.
Apply Na√Øve Bayes
Go to Classify tab ‚Üí Select classifier:
weka.classifiers.bayes.NaiveBayes
Choose Cross-validation (10-fold) or Percentage split (e.g., 70:30).
Evaluate
WEKA will output:
Confusion Matrix
Accuracy, Precision, Recall, F1-score
‚úÖ Example Confusion Matrix (spam dataset):
  === Confusion Matrix ===
    a   b   <-- classified as
   450  20 | a = ham
    30 500 | b = spam
450 = True Ham
500 = True Spam
20 = Ham misclassified as Spam (False Positive)
30 = Spam misclassified as Ham (False Negative)

13.Outlier Detection
Identify outliers in a dataset using distance-based methods or Z-score analysis. Visualize the result using scatter plots.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors
from scipy.stats import zscore
Step 1: Create synthetic dataset
np.random.seed(42)
Generate normal cluster of points
x = np.random.normal(5, 1, 100)
y = np.random.normal(5, 1, 100)
Add some clear outliers
x_outliers = np.random.uniform(10, 15, 5)
y_outliers = np.random.uniform(10, 15, 5)
Combine data
x = np.concatenate([x, x_outliers])
y = np.concatenate([y, y_outliers])
data = pd.DataFrame({"x": x, "y": y})
Method 1: Z-Score Analysis
z_scores = np.abs(zscore(data))  # compute z-scores for each column
outliers_z = (z_scores > 3).any(axis=1)  # mark as outlier if z > 3
Method 2: Distance-Based Outlier Detection (kNN)
nbrs = NearestNeighbors(n_neighbors=5).fit(data)
distances, indices = nbrs.kneighbors(data)
mean distance to k nearest neighbors
mean_distances = distances.mean(axis=1)
threshold = np.percentile(mean_distances, 95)  # top 5% distances = outliers
outliers_dist = mean_distances > threshold
Visualization
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
Z-score method plot
axes[0].scatter(data["x"], data["y"], c="blue", label="Normal")
axes[0].scatter(data[outliers_z]["x"], data[outliers_z]["y"], c="red", label="Outliers")
axes[0].set_title("Outlier Detection using Z-Score")
axes[0].legend()
Distance-based method plot
axes[1].scatter(data["x"], data["y"], c="blue", label="Normal")
axes[1].scatter(data[outliers_dist]["x"], data[outliers_dist]["y"], c="orange", label="Outliers")
axes[1].set_title("Outlier Detection using Distance-Based Method")
axes[1].legend()
plt.show()
What this does:
Creates a dataset with a cluster + outliers.
 
Detects outliers with:
Z-score (statistical method, threshold = 3)
Distance-based (kNN) (points far from neighbors are flagged).
Visualizes both methods side-by-side using scatter plots.

14.Correlation and Data Visualization
Use Python (Seaborn, Matplotlib) or Power BI to find and visualize the correlation between variables in a dataset.
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
Step 1: Load Dataset
Example: Iris dataset
from sklearn.datasets import load_iris
iris = load_iris(as_frame=True)
df = iris.frame  # pandas DataFrame
Step 2: Compute Correlation Matrix
corr_matrix = df.corr()
print("Correlation Matrix:\n", corr_matrix)
Step 3: Visualization
Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap of Iris Dataset")
plt.show()
Pairplot (scatter plot matrix to see relationships)
sns.pairplot(df, diag_kind="kde")
plt.show()

15.Model Evaluation Techniques
Compare different classification models (Decision Tree, SVM, KNN) using precision, recall, F1-score, and ROC curves on a medical dataset.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_curve, auc, RocCurveDisplay
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
Step 1: Load Dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)
 
Step 2: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.3, random_state=42, stratify=y
)
# Scale features (important for SVM & KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
Step 3: Define Models
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "SVM": SVC(probability=True, kernel="rbf", random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}
4: Train, Evaluate, Compare
results = {}
plt.figure(figsize=(8, 6))
for name, model in models.items():
Train
model.fit(X_train_scaled, y_train)
Predictions
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)[:, 1]
Classification report (precision, recall, f1)
print(f"\n{name} Classification Report:\n")    print(classification_report(y_test, y_pred, target_names=data.target_names))
ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], 'k--')  # diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend(loc="lower right")
plt.show()
